{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Crash Course"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# package for progress bars\n",
    "from tqdm import tqdm\n",
    "from mushroom_dataloader import enumerated_data, numerize_data, one_hot, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641fb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Where to run your model ####\n",
    "# if you have a gpu you would like to run your model on the gpu for shorter runtime:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71c2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### LOAD DATA ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9e6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust to your folder structure\n",
    "\n",
    "PATH = './mushroom_data/agaricus-lepiota.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bd0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH, 'r') as f:\n",
    "    data = [el.strip().split(',') for el in f.readlines()]\n",
    "input_data = [el[1:] for el in data] # list1(list2); list2 consists of 23 features for each mushroom \n",
    "output_data = [el[0] for el in data] # list(str); contains the output values [e, p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2208c7eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_instance_number = int(len(input_data) * 0.7)\n",
    "\n",
    "train_input = input_data[:train_instance_number]\n",
    "train_output = output_data[:train_instance_number]\n",
    "\n",
    "test_input = input_data[train_instance_number:]\n",
    "test_output = output_data[train_instance_number:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d8d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt data to Torch requirements (make DataLoader) #\n",
    "\n",
    "train_inp = numerize_data(train_input)\n",
    "train_out = one_hot(train_output)\n",
    "train_dataset = Dataset(enumerated_data(train_inp, train_out))\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "test_inp = numerize_data(test_input)\n",
    "test_out = one_hot(test_output)\n",
    "test_dataset = Dataset(enumerated_data(test_inp, test_out))\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed11e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in dataloader:\n",
    "#     print(el)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e253d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Define Model Class #\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0798fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MushroomClassifier(nn.Module):        # inherit from PyTorch model class (important!)\n",
    "    \"\"\"\n",
    "    A classifier for the mushroom dataset\n",
    "    Predicts for a tensor describing the attributes of a mushroom whether the mushroom is edible or poisonous\n",
    "    A 2 layer Feedforward Network with Sigmoid activation function (i.e. sigmoid non linearity)\n",
    "    \"\"\"\n",
    "\n",
    "    ## init function\n",
    "    # needs to get all parameters that your model should have -> defines the layers / structure of your model\n",
    "    def __init__(self,\n",
    "                input_size: int,\n",
    "                number_classes: int,\n",
    "                hidden_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_size: the size of the input layer; needs to match the length of an input tensor\n",
    "        :param number_classes: the number of different classes (outputs); will be the output dimension\n",
    "        :param hidden_size: the size of the hidden layer\n",
    "        \"\"\"\n",
    "        super(MushroomClassifier, self).__init__()          # important!\n",
    "        \n",
    "        # define each of the layers of your model: type of layer, dimensions\n",
    "        # each output of a layer is the input to the next layer (see Slide 12)\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)      # [22, 128]\n",
    "        self.layer2 = nn.Linear(hidden_size, number_classes)  # [128, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## forward function\n",
    "    def forward(self, \n",
    "                input_batch):\n",
    "        \"\"\"\n",
    "        gets an input batch and feeds it through the layers of the network to get the output predictions for each input\n",
    "        :param input_batch: a batch of tensors corresponding to the data; shape: [batch_size, input_length]\n",
    "        :return: the predictions of the model; shape: [batch_size, number_classes]\n",
    "        \"\"\"\n",
    "        # feed input batch into the first layer\n",
    "        out_first_layer = self.layer1(input_batch)\n",
    "\n",
    "        # apply (sigmoid) activation function -> this is the \"nonlinearity\" mentioned on Slide 12\n",
    "        # does not change the dimension\n",
    "        out_first_activation = torch.sigmoid(out_first_layer)\n",
    "\n",
    "        # feed output of activation function to the second layer\n",
    "        out_sec_layer = self.layer2(out_first_activation)\n",
    "\n",
    "        return out_sec_layer\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b48ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MushroomClassifier(\n  (layer1): Linear(in_features=22, out_features=128, bias=True)\n  (layer2): Linear(in_features=128, out_features=2, bias=True)\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# Initialize Model #\n",
    "####################\n",
    "\n",
    "# create an instance of the model you would like to train\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "input_size = len(train_input[0])\n",
    "num_classes = 2\n",
    "\n",
    "classifier = MushroomClassifier(input_size=input_size, number_classes=num_classes,\n",
    "                                hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# move model to device (default - CPU)\n",
    "# if you only use a cpu you do not have to call .to(device) but if you want to run on the gpu you have to do this for both your model and the tensors (see training function)\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f36b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Set Hyperparameter Values #\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2ab5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Parameters ###\n",
    "\n",
    "# number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# the learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# the optimizer to use\n",
    "OPTIMIZER = optim.SGD           # important: no brackets if you do not directly define it with the required parameters\n",
    "\n",
    "# loss function \n",
    "LOSS_FUNCTION = nn.functional.mse_loss      # important: no brackets if you do not directly define it with the required parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d320faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Training loop #\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f48127c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module, \n",
    "                train_data: DataLoader,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    runs one commplete training run, i.e. trains the model on your training data for\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs to train\n",
    "    :param optimizer_type: the type of optimizer to use for training\n",
    "    :param loss_function: the type of loss function to use\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # Important: bring model into training mode\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run training for specified number of epochs; use tqdm to keep track of progress / estimated run time \n",
    "    for epoch in tqdm(range(num_epochs), desc='Classifier Training\\n'):\n",
    "        \n",
    "        print(f'---------- Started Epoch {epoch} -----------')\n",
    "\n",
    "        for batch in train_data:\n",
    "\n",
    "            # get the input instances (and move them to the device you use)\n",
    "            input_attributes = batch[0].to(device)\n",
    "            # get the corresponding labels\n",
    "            gold_labels = batch[1].to(device)\n",
    "\n",
    "\n",
    "            # compute model predictions with current model parameters\n",
    "            model_output = model(input_attributes)\n",
    "\n",
    "            # Compute Loss for current batch\n",
    "            loss = loss_function(model_output, gold_labels)\n",
    "\n",
    "\n",
    "            #Important: otherwise you add up your gradients for all batches and for all epochs\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # the training loop function does not return anything because the model object gets changed itself\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b64100",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Start Training ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      ":   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 0 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      ":  20%|██        | 1/5 [00:03<00:14,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 1 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      ":  40%|████      | 2/5 [00:06<00:10,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 2 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      ":  60%|██████    | 3/5 [00:10<00:06,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 3 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      ":  80%|████████  | 4/5 [00:13<00:03,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 4 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      "Classifier Training5/5 [00:17<00:00,  3.42s/it]\n",
      ": 100%|██████████| 5/5 [00:17<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Train the model #\n",
    "###################\n",
    "\n",
    "# run a complete training loop\n",
    "train_model(model=classifier, train_data=train_dataloader, num_epochs=NUM_EPOCHS,\n",
    "            optimizer_type=OPTIMIZER, loss_function=LOSS_FUNCTION, learning_rate=LEARNING_RATE)\n",
    "\n",
    "# now the model object you defined above in the initialization cell is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9478181",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Evaluation #\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea89db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MushroomClassifier(\n",
       "  (layer1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this is not runnable because it actually does nothing but only illustrates two important steps you need to include\n",
    "# bring model in evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():   # important: otherwise you will compute gradients while running the model on your test data\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        print(f'--------- Evaluate Model ------------')\n",
    "\n",
    "        # run trained model on test instances\n",
    "\n",
    "        # compute evaluation metrics to evaluate the model performance based on predictions of the model\n",
    "\n",
    "# bring back into train mode again\n",
    "classifier.train() # not necessary if you set your model into train mode during the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7559f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Save and Load Model Parameters #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eca10bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model parameters in a .pt file\n",
    "torch.save(classifier.state_dict(), \"./model_parameters_mushroom.pt\")\n",
    "\n",
    "# load trained model parameters again\n",
    "\n",
    "# first create an instance of the model class\n",
    "trained_classifier = MushroomClassifier(input_size=input_size, number_classes=num_classes, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# then load the trained parameters\n",
    "trained_classifier.load_state_dict(torch.load(\"./model_parameters_mushroom.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}