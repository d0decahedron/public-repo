{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e8606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# package for progress bars\n",
    "from tqdm import tqdm\n",
    "from mushroom_dataloader import enumerated_data, numerize_data, one_hot, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641fb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Where to run your model ####\n",
    "# if you have a gpu you would like to run your model on the gpu for shorter runtime:\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71c2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### LOAD DATA ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9e6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mushroom_data/agaricus-lepiota.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bd0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH, 'r') as f:\n",
    "    data = [el.strip().split(',') for el in f.readlines()]\n",
    "input_data = [el[1:] for el in data] # list1(list2); list2 consists of 23 features for each mushroom \n",
    "output_data = [el[0] for el in data] # list(str); contains the output values [e, p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2208c7eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_instance_number = int(len(input_data) * 0.7)\n",
    "\n",
    "train_input = input_data[:train_instance_number]\n",
    "train_output = output_data[:train_instance_number]\n",
    "\n",
    "test_input = input_data[train_instance_number:]\n",
    "test_output = output_data[train_instance_number:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d8d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt data to Torch requirements (make DataLoader) #\n",
    "\n",
    "train_inp = numerize_data(train_input)\n",
    "train_out = one_hot(train_output)\n",
    "train_dataset = Dataset(enumerated_data(train_inp, train_out))\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "test_inp = numerize_data(test_input)\n",
    "test_out = one_hot(test_output)\n",
    "test_dataset = Dataset(enumerated_data(test_inp, test_out))\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed11e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in dataloader:\n",
    "#     print(el)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e253d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Define Model Class #\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0798fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MushroomClassifier(nn.Module):        # inherit from PyTorch model class (important!)\n",
    "    \"\"\"\n",
    "    A classifier for the mushroom dataset\n",
    "    Predicts for a tensor describing the attributes of a mushroom whether the mushroom is edible or poisonous\n",
    "    A 2 layer Feedforward Network with ReLU activation funct\n",
    "    \"\"\"\n",
    "\n",
    "    ## init function\n",
    "    # needs to get all parameters that your model should have \n",
    "    def __init__(self,\n",
    "                input_size: int,\n",
    "                number_classes: int,\n",
    "                hidden_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_size: the size of the input layer; needs to match the length of an input tensor\n",
    "        :param number_classes: the number of different classes (outputs); will be the output dimension\n",
    "        :param hidden_size: the size of the hidden layer\n",
    "        \"\"\"\n",
    "        super(MushroomClassifier, self).__init__()          # important!\n",
    "        \n",
    "        # define each of the layers of your model: type of layer, dimensions\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)      # [22, 128]\n",
    "        self.layer2 = nn.Linear(hidden_size, number_classes)  # [128, 2]\n",
    "\n",
    "\n",
    "\n",
    "        #Alternative: adding a third layer of a smaller size\n",
    "        #self.layer2 = nn.Linear(hidden_size, hidden_size_2) # [128, 54]\n",
    "        #self.layer3 = nn.Linear(hidden_size_2, number_classes) # [54, 2]\n",
    "\n",
    "\n",
    "    ## forward function\n",
    "    def forward(self, \n",
    "                input_batch):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_batch: a batch of tensors corresponding to the data; shape: [batch_size, input_length]\n",
    "        :return: the predictions of the model; shape: [batch_size, number_classes]\n",
    "        \"\"\"\n",
    "        # feed input batch into the first layer\n",
    "        out_first_layer = self.layer1(input_batch)\n",
    "\n",
    "        # apply (sigmoid) activation function\n",
    "        out_first_activation = nn.functional.sigmoid(out_first_layer)\n",
    "\n",
    "        # feed output of activation function to the second layer\n",
    "        out_sec_layer = self.layer2(out_first_activation)\n",
    "\n",
    "        return out_sec_layer\n",
    "        \n",
    "        #         FOR ALTERNATIVE:\n",
    "        #  Applying the activation function to second layer output; passing it to the third layer; returning the output of the third layer\n",
    "        #out_sec_activation = nn.functional.relu(out_sec_layer)\n",
    "        #out_third_layer = self.layer3(out_sec_activation)\n",
    "        \n",
    "#        return out_sec_layer\n",
    "        #return out_third_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4b48ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MushroomClassifier(\n",
       "  (layer1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# Initialize Model #\n",
    "####################\n",
    "\n",
    "# create an instance of the model you would like to train\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "input_size = len(train_input[0])\n",
    "num_classes = 2\n",
    "\n",
    "classifier = MushroomClassifier(input_size=input_size, number_classes=num_classes,\n",
    "                                hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# move model to device (default - CPU)\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f36b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Set Hyperparameter Values #\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ab5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Parameters ###\n",
    "\n",
    "# number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# the learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# the optimizer to use\n",
    "OPTIMIZER = optim.SGD\n",
    "\n",
    "# loss function \n",
    "LOSS_FUNCTION = nn.functional.mse_loss\n",
    "\n",
    "# number of instances per batch\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d320faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Training loop #\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48127c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module, \n",
    "                train_data: DataLoader,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs to train\n",
    "    :param optimizer_type: the type of optimizer to use for training\n",
    "    :param loss_function: the type of loss function to use\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # TODO: remove\n",
    "    batch_losses = []\n",
    "\n",
    "    # Important: bring model into training mode\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run training for specified number of epochs; use tqdm to keep track of progress / estimated run time \n",
    "    for epoch in tqdm(range(num_epochs), desc='Classifier Training\\n'):\n",
    "        \n",
    "        print(f'---------- Started Epoch {epoch} -----------')\n",
    "\n",
    "        for batch in train_data:\n",
    "\n",
    "            # get the input instances (and move them to the device you use)\n",
    "            input_attributes = batch[0].to(device)\n",
    "            # get the corresponding labels\n",
    "            gold_labels = batch[1].to(device)\n",
    "\n",
    "\n",
    "            # compute model predictions with current model parameters\n",
    "            model_output = model(input_attributes)\n",
    "\n",
    "            # Compute Loss for current batch\n",
    "            loss = loss_function(model_output, gold_labels)\n",
    "\n",
    "            #print(f'Training Loss: {loss} \\n')\n",
    "            #print(type(loss))\n",
    "\n",
    "            #TODO: remove\n",
    "            batch_losses.append(float(loss))\n",
    "\n",
    "            #Important: otherwise you add up your gradients for all batches\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "         # report mean loss of epoch\n",
    "        average_loss = sum(batch_losses) / len(batch_losses)\n",
    "        print(f'\\nAverage loss on training data: {average_loss}')\n",
    "\n",
    "\n",
    "        # the training loop function does not return anything because the model object gets changed itself\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b64100",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Start Training ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   0%|                               | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 0 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  20%|████▌                  | 1/5 [00:03<00:13,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss on training data: 0.03779443117974892\n",
      "---------- Started Epoch 1 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  40%|█████████▏             | 2/5 [00:06<00:09,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss on training data: 0.03650831367259234\n",
      "---------- Started Epoch 2 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  60%|█████████████▊         | 3/5 [00:09<00:06,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss on training data: 0.035346548067031254\n",
      "---------- Started Epoch 3 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  80%|██████████████████▍    | 4/5 [00:12<00:03,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss on training data: 0.03430127108056262\n",
      "---------- Started Epoch 4 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      "Classifier Training███████████| 5/5 [00:15<00:00,  3.18s/it]\n",
      ": 100%|███████████████████████| 5/5 [00:15<00:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss on training data: 0.03336245147667138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Train the model #\n",
    "###################\n",
    "\n",
    "# run a complete training loop\n",
    "train_model(model=classifier, train_data=train_dataloader, num_epochs=NUM_EPOCHS,\n",
    "            optimizer_type=OPTIMIZER, loss_function=LOSS_FUNCTION, learning_rate=LEARNING_RATE)\n",
    "\n",
    "# now the model object you defined above in the initialization cell is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9478181",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Evaluation #\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea89db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MushroomClassifier(\n",
       "  (layer1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring model in evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "# important: otherwise you will compute gradients while running the model on your test data\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        test_instance = batch[0]\n",
    "        test_target = batch[1]\n",
    "\n",
    "        # run model on test instances\n",
    "        # compute evaluation metrics\n",
    "\n",
    "# bring back into train mode again\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7559f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Save and Load Model Parameters #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eca10bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model parameters in a .pt file\n",
    "torch.save(classifier.state_dict(), \"./results/model_parameters_mushroom.pt\")\n",
    "\n",
    "# load trained model parameters again\n",
    "\n",
    "# first create an instance of the model class\n",
    "trained_classifier = MushroomClassifier(input_size=input_size, number_classes=num_classes, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "# then load the trained parameters\n",
    "trained_classifier.load_state_dict(torch.load(\"./results/model_parameters_mushroom.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
